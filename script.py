#!/usr/bin/env python3
"""
Vislona RAG Chatbot Setup Script
This script helps you set up the environment and dependencies for the Vislona chatbot.
"""

import os
import sys
import subprocess
import requests
import json
from pathlib import Path
import time

def run_command(command, description=""):
    """Run a system command and return success status"""
    print(f"üîÑ {description if description else f'Running: {command}'}")
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=60)
        if result.returncode == 0:
            print("‚úÖ Success!")
            return True, result.stdout
        else:
            print(f"‚ùå Error: {result.stderr}")
            return False, result.stderr
    except subprocess.TimeoutExpired:
        print(f"‚è±Ô∏è Timeout: Command took too long")
        return False, "Timeout"
    except Exception as e:
        print(f"‚ùå Exception: {e}")
        return False, str(e)

def check_python_version():
    """Check if Python version is compatible"""
    print("üêç Checking Python version...")
    version = sys.version_info
    if version.major == 3 and version.minor >= 8:
        print(f"‚úÖ Python {version.major}.{version.minor}.{version.micro} is compatible")
        return True
    else:
        print(f"‚ùå Python {version.major}.{version.minor}.{version.micro} is not compatible")
        print("üí° Please install Python 3.8 or higher")
        return False

def check_ollama_installation():
    """Check if Ollama is installed"""
    print("ü¶ô Checking Ollama installation...")
    success, output = run_command("ollama --version", "Checking Ollama version")

    if success:
        print(f"‚úÖ Ollama is installed: {output.strip()}")
        return True
    else:
        print("‚ùå Ollama is not installed")
        print("üí° Install Ollama from: https://ollama.ai/download")
        print("üí° Or use the following commands:")
        print("   ‚Ä¢ Windows: Download from https://ollama.ai/download")
        print("   ‚Ä¢ macOS: brew install ollama")
        print("   ‚Ä¢ Linux: curl -fsSL https://ollama.ai/install.sh | sh")
        return False

def check_ollama_running():
    """Check if Ollama server is running"""
    print("üåê Checking Ollama server status...")
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"‚úÖ Ollama server is running with {len(models)} models")
            return True, models
        else:
            print("‚ùå Ollama server is not responding correctly")
            return False, []
    except requests.exceptions.RequestException:
        print("‚ùå Ollama server is not running")
        print("üí° Start Ollama with: ollama serve")
        return False, []

def install_python_packages():
    """Install required Python packages"""
    print("üì¶ Installing Python packages...")

    packages = [
        "streamlit",
        "langchain",
        "langchain-community",
        "langchain-ollama",
        "faiss-cpu",
        "requests",
        "pathlib"
    ]

    for package in packages:
        print(f"üì• Installing {package}...")
        success, output = run_command(f"pip install {package}", f"Installing {package}")
        if not success:
            print(f"‚ùå Failed to install {package}")
            return False

    print("‚úÖ All Python packages installed successfully!")
    return True

def download_ollama_models():
    """Download recommended Ollama models"""
    print("üéØ Setting up Ollama models...")

    # Check if Ollama is running
    running, existing_models = check_ollama_running()
    if not running:
        print("‚ùå Please start Ollama server first: ollama serve")
        return False

    existing_model_names = [model['name'] for model in existing_models] if existing_models else []

    # Recommended models
    models_to_install = [
        ("nomic-embed-text", "Embedding model for vector search"),
        ("llama3.2:1b", "Lightweight chat model (recommended for testing)"),
    ]

    # Optional models
    optional_models = [
        ("llama3.2:3b", "Better quality chat model"),
        ("llama3.1:8b", "High quality chat model (requires more resources)"),
    ]

    # Install essential models
    for model, description in models_to_install:
        if model in existing_model_names:
            print(f"‚úÖ {model} already installed")
            continue

        print(f"üì• Installing {model} - {description}")
        print("‚è±Ô∏è This may take a few minutes...")

        success, output = run_command(f"ollama pull {model}", f"Downloading {model}")
        if success:
            print(f"‚úÖ {model} installed successfully!")
        else:
            print(f"‚ùå Failed to install {model}")
            print("üí° You can install it manually later with: ollama pull", model)

    # Ask about optional models
    print("\nü§î Optional models (better quality but larger size):")
    for model, description in optional_models:
        if model in existing_model_names:
            print(f"‚úÖ {model} already installed")
            continue

        response = input(f"Install {model} - {description}? (y/N): ").strip().lower()
        if response == 'y':
            print(f"üì• Installing {model}...")
            success, output = run_command(f"ollama pull {model}", f"Downloading {model}")
            if success:
                print(f"‚úÖ {model} installed successfully!")
            else:
                print(f"‚ùå Failed to install {model}")
        else:
            print(f"‚è≠Ô∏è Skipping {model}")

    return True

def create_project_structure():
    """Create necessary project directories and files"""
    print("üìÅ Creating project structure...")

    # Create directories
    directories = [
        "data",
        "models",
        "logs",
        "vector_stores"
    ]

    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"‚úÖ Created directory: {directory}")

    # Create requirements.txt
    requirements = """streamlit>=1.28.0
langchain>=0.3.0
langchain-community>=0.3.0
langchain-ollama>=0.2.0
faiss-cpu>=1.7.4
requests>=2.31.0
pathlib
"""

    with open("requirements.txt", "w") as f:
        f.write(requirements)
    print("‚úÖ Created requirements.txt")

    # Create .gitignore
    gitignore = """# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
.env

# Streamlit
.streamlit/

# Vector stores
faiss_index_*
*.faiss
*.pkl

# Logs
logs/
*.log

# Data
data/
models/

# IDE
.vscode/
.idea/
*.swp
*.swo
"""

    with open(".gitignore", "w") as f:
        f.write(gitignore)
    print("‚úÖ Created .gitignore")

    return True

def create_launch_scripts():
    """Create convenient launch scripts"""
    print("üöÄ Creating launch scripts...")

    # Windows batch script
    windows_script = """@echo off
echo Starting Vislona AI Chatbot...
echo.

REM Check if Ollama is running
curl -s http://localhost:11434/api/tags > nul 2>&1
if %errorlevel% neq 0 (
    echo Ollama is not running. Starting Ollama...
    start /b ollama serve
    timeout /t 3 > nul
)

REM Start Streamlit
streamlit run vislona_chatbot.py

pause
"""

    with open("start_chatbot.bat", "w") as f:
        f.write(windows_script)
    print("‚úÖ Created start_chatbot.bat (Windows)")

    # Unix shell script
    unix_script = """#!/bin/bash
echo "Starting Vislona AI Chatbot..."
echo

# Check if Ollama is running
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "Ollama is not running. Starting Ollama..."
    ollama serve &
    sleep 3
fi

# Start Streamlit
streamlit run vislona_chatbot.py
"""

    with open("start_chatbot.sh", "w") as f:
        f.write(unix_script)

    # Make shell script executable
    try:
        os.chmod("start_chatbot.sh", 0o755)
        print("‚úÖ Created start_chatbot.sh (Linux/macOS)")
    except:
        print("‚úÖ Created start_chatbot.sh (set executable permissions manually)")

    return True

def run_tests():
    """Run basic tests to verify installation"""
    print("üß™ Running installation tests...")

    # Test Python imports
    test_imports = [
        "streamlit",
        "langchain",
        "langchain_community.vectorstores",
        "langchain_ollama",
        "faiss",
        "requests"
    ]

    for module in test_imports:
        try:
            __import__(module)
            print(f"‚úÖ {module} import successful")
        except ImportError as e:
            print(f"‚ùå {module} import failed: {e}")
            return False

    # Test Ollama connection
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama connection successful")
        else:
            print("‚ùå Ollama connection failed")
            return False
    except:
        print("‚ö†Ô∏è Ollama connection test skipped (server not running)")

    print("‚úÖ All tests passed!")
    return True

def main():
    """Main setup function"""
    print("ü§ñ Vislona RAG Chatbot Setup")
    print("=" * 50)

    # Step 1: Check Python version
    if not check_python_version():
        return False

    # Step 2: Check Ollama installation
    if not check_ollama_installation():
        print("\n‚èπÔ∏è Setup paused. Please install Ollama first.")
        return False

    # Step 3: Install Python packages
    print("\n" + "=" * 30)
    install_packages = input("üì¶ Install Python packages? (Y/n): ").strip().lower()
    if install_packages != 'n':
        if not install_python_packages():
            print("‚ùå Package installation failed")
            return False

    # Step 4: Create project structure
    print("\n" + "=" * 30)
    create_structure = input("üìÅ Create project structure? (Y/n): ").strip().lower()
    if create_structure != 'n':
        create_project_structure()

    # Step 5: Download Ollama models
    print("\n" + "=" * 30)
    download_models = input("üéØ Download Ollama models? (Y/n): ").strip().lower()
    if download_models != 'n':
        download_ollama_models()

    # Step 6: Create launch scripts
    print("\n" + "=" * 30)
    create_scripts = input("üöÄ Create launch scripts? (Y/n): ").strip().lower()
    if create_scripts != 'n':
        create_launch_scripts()

    # Step 7: Run tests
    print("\n" + "=" * 30)
    run_test = input("üß™ Run installation tests? (Y/n): ").strip().lower()
    if run_test != 'n':
        run_tests()

    print("\n" + "=" * 50)
    print("üéâ Setup completed!")
    print("\nüìã Next steps:")
    print("1. Save the chatbot code as 'vislona_chatbot.py'")
    print("2. Start Ollama server: ollama serve")
    print("3. Run the chatbot: streamlit run vislona_chatbot.py")
    print("4. Or use launch scripts: ./start_chatbot.sh (Linux/macOS) or start_chatbot.bat (Windows)")
    print("\nüí° Tips:")
    print("‚Ä¢ The chatbot will create a demo vector store on first run")
    print("‚Ä¢ You can add your own documents to create a custom knowledge base")
    print("‚Ä¢ Check the sidebar for configuration options")

    return True

if __name__ == "__main__":
    try:
        success = main()
        if success:
            print("\n‚úÖ Setup completed successfully!")
        else:
            print("\n‚ùå Setup encountered issues. Please check the errors above.")
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è Setup interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        print("üí° Please check your Python environment and try again")